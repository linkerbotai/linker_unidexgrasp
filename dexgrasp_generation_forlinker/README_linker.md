# Dexterous Grasping Proposal Generation Code for UniDexGrasp on linkerbotai's Linker L20 robot hand.


We follow the work on the [UniDexGrasp](https://arxiv.org/abs/2303.00938) generation section, using a mapping method to transfer the ShadowHand grasping poses generated by the model to the L20 hand, and have supplemented the relevant content.


## Installation

Our code is tested on (Ubuntu 20.04).

* Clone this repository:
```commandline
git clone https://github.com/PKU-EPIC/UniDexGrasp.git
cd UniDexGrasp
```

* Create a [conda](https://www.anaconda.com/) environment and activate it:
```commandline
conda create -n unidexgrasp python=3.8
conda activate unidexgrasp
```

* Install the dependencies:
```commandline
conda install -y pytorch==1.10.0 torchvision==0.11.0 torchaudio==0.10.0 cudatoolkit=11.3 -c pytorch -c conda-forge
conda install -y https://mirrors.bfsu.edu.cn/anaconda/cloud/pytorch3d/linux-64/pytorch3d-0.6.2-py38_cu113_pyt1100.tar.bz2
pip install -r requirements.txt
cd thirdparty/pytorch_kinematics
pip install -e .
cd ../nflows
pip install -e .
cd ../
git clone https://github.com/wrc042/CSDF.git
cd CSDF
pip install -e .
cd ../../
```


## Data

1. Create a `data` folder under `dexgrasp_generation`:

```commandline
mkdir data
```
2. Download the data from [here](https://mirrors.pku.edu.cn/dl-release/UniDexGrasp_CVPR2023/), and put them under `data`. Specifically, you need `mjcf` to build the ShadowHand, and `DFCData` contains the grasp labels.

<details>
  <summary> Click to see the file structure </summary>
  
  ```commandline
  UniDexGrasp
  ├── dexgrasp_generation
  │   ├── data
  │   │   ├── DFCdata
  │   │   │    ├── datasetv4.1（same as policy)
  │   │   │    ├── meshdatav3  (same items as policy,but including point cloud)
  │   │   │    └── splits
  │   │   └── mjcf
  │   └── ...
  ├── dexgrasp_policy
  │   ├── assets
  │   │   ├── meshdatav3_pc_feat
  │   │   ├── meshdatav3_pc_fps 
  │   │   ├── meshdatav3_scaled
  │   │   ├── datasetv4.1 
  │   │   └── mjcf
  │   └── ...
  └──
  ```
</details>

## Training

### GraspIPDF

```commandline
python ./network/train.py --config-name ipdf_config \
                          --exp-dir ./ipdf_train
```

### GraspGlow

```commandline
python ./network/train.py --config-name glow_config \
                          --exp-dir ./glow_train
python ./network/train.py --config-name glow_joint_config \
                          --exp-dir ./glow_train
```

### ContactNet

```commandline
python ./network/train.py --config-name cm_net_config \
                          --exp-dir ./cm_net_train
```

## Evaluation
With the pretrained model in ./runs, you can try from here.

```commandline
python ./network/eval.py  --config-name eval_config \
                          --exp-dir=./eval
```

## Mapping

With the result in ./eval,you can also try from here.

### visulization

```commandline
python ./tests/visualize_result_l20_shadow.py --exp_dir 'eval' --num 3
```

<img  src="/readme_image/item_1.jpg" width="550">
<img  src="/readme_image/item_2.jpg" width="550">

### save for policy
Save the mapped results in a format suitable for subsequent policy training.Change the 'pt_file_path' and the 'save_dir'
```commandline
python ./tests/data_for_RL.py
```




## Acknowledgements

* [PointNet++](https://github.com/rusty1s/pytorch_geometric)
* [Implicit PDF](https://github.com/google-research/google-research/tree/master/implicit_pdf)
* [CSDF](https://github.com/wrc042/CSDF)
* [nkolot's implementation of nflows](https://github.com/nkolot/nflows)
